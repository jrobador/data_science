{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing cross-validated metrics\n",
    "How to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data,\n",
    "fitting a model and computing the score 5 consecutive times (with different splits each time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "0.98 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "print(scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96658312 1.         0.96658312 0.96658312 1.        ]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    clf, X, y, cv=5, scoring='f1_macro')\n",
    "\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    clf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find all metrics from sklearn as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "adjusted_mutual_info_score\n",
      "adjusted_rand_score\n",
      "average_precision\n",
      "balanced_accuracy\n",
      "completeness_score\n",
      "explained_variance\n",
      "f1\n",
      "f1_macro\n",
      "f1_micro\n",
      "f1_samples\n",
      "f1_weighted\n",
      "fowlkes_mallows_score\n",
      "homogeneity_score\n",
      "jaccard\n",
      "jaccard_macro\n",
      "jaccard_micro\n",
      "jaccard_samples\n",
      "jaccard_weighted\n",
      "matthews_corrcoef\n",
      "max_error\n",
      "mutual_info_score\n",
      "neg_brier_score\n",
      "neg_log_loss\n",
      "neg_mean_absolute_error\n",
      "neg_mean_absolute_percentage_error\n",
      "neg_mean_gamma_deviance\n",
      "neg_mean_poisson_deviance\n",
      "neg_mean_squared_error\n",
      "neg_mean_squared_log_error\n",
      "neg_median_absolute_error\n",
      "neg_negative_likelihood_ratio\n",
      "neg_root_mean_squared_error\n",
      "normalized_mutual_info_score\n",
      "positive_likelihood_ratio\n",
      "precision\n",
      "precision_macro\n",
      "precision_micro\n",
      "precision_samples\n",
      "precision_weighted\n",
      "r2\n",
      "rand_score\n",
      "recall\n",
      "recall_macro\n",
      "recall_micro\n",
      "recall_samples\n",
      "recall_weighted\n",
      "roc_auc\n",
      "roc_auc_ovo\n",
      "roc_auc_ovo_weighted\n",
      "roc_auc_ovr\n",
      "roc_auc_ovr_weighted\n",
      "top_k_accuracy\n",
      "v_measure_score\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "for metric in metrics.get_scorer_names():\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information: \n",
    "1. https://scikit-learn.org/stable/api/sklearn.metrics.html\n",
    "2. https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INRIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
